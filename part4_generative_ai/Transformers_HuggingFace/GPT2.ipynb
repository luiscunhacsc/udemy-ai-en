{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiscunhacsc/udemy-ai-en/blob/main/part4_generative_ai/Pretrained_Transformers_HuggingFace/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_wj4_AuAqt_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Suppress warnings (to make output cleaner)\n",
        "# We are using public models, so no need to authenticate\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqCmEZ4M8fCH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install the necessary libraries\n",
        "%pip install -q transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvcpJlp_8hFU"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA593vkw6y4K"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Function for text generation\n",
        "\n",
        "def generate_text(prompt, model, tokenizer, max_length=100, temperature=0.7):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
        "    attention_mask = torch.ones(inputs.shape, dtype=torch.long).to('cuda')\n",
        "\n",
        "    # Measure generation time\n",
        "    start_time = time.time()\n",
        "    outputs = model.generate(inputs,\n",
        "                             attention_mask=attention_mask,\n",
        "                             max_length=max_length,\n",
        "                             temperature=temperature,\n",
        "                             num_return_sequences=1,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             pad_token_id=tokenizer.eos_token_id)  # Explicitly set the pad_token_id\n",
        "    end_time = time.time()\n",
        "\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    generation_time = end_time - start_time\n",
        "\n",
        "    return text, generation_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPd44l_sGB_0",
        "outputId": "f17c18df-4937-4894-f008-8add3e99242d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Narrativa com temperatura 0.2 ---\n",
            "Tempo de geração: 1.99 segundos\n",
            "Once upon a time in Hollywood, the director of a film was a man who had been a member of the family of a wealthy family. He was a man who had been a member of the family of a wealthy family. He was a man who had been a member of the family of a wealthy family. He was a man who had been a member of the family of a wealthy family. He was a man who had been a member of the family of a wealthy family. He was a man\n",
            "\n",
            "--- Narrativa com temperatura 0.7 ---\n",
            "Tempo de geração: 0.90 segundos\n",
            "Once upon a time in Hollywood, the idea of a young actor being cast for the big screen was something that had been a little more taboo until suddenly the new generation had become very popular. The idea that this kind of thing was going to happen was really something that had been a little bit taboo until suddenly the new generation had become very popular.\n",
            "\n",
            "--- Narrativa com temperatura 1.0 ---\n",
            "Tempo de geração: 1.59 segundos\n",
            "Once upon a time in Hollywood he came to one of the very first houses in Hollywood's history called Hollywood Studios and had his house there built. He said that this was where, of all of the American cities all over America, he got his idea of a scene when he did a scene on the first scene which was with a guy from New York. And I like to call this one Scene. Where everybody is like, \"Yeah, this is where I'm going to meet the best actress in\n"
          ]
        }
      ],
      "source": [
        "# Initial prompt\n",
        "prompt = \"Once upon a time in Hollywood\"\n",
        "\n",
        "# Generation with different temperatures\n",
        "for temp in [0.2, 0.7, 1.0]:\n",
        "    print(f\"\\n--- Narrative with temperature {temp} ---\")\n",
        "    narrative, gen_time = generate_text(prompt, model, tokenizer, temperature=temp)\n",
        "    print(f\"Generation time: {gen_time:.2f} seconds\")\n",
        "    print(narrative)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNqAn+16I6zZi82hOnXiJrA",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
