# README

This repository contains three notebooks that explore different language generation models based on deep neural networks. Through these examples, we seek to understand the fundamental principles that underpin larger and more complex models like ChatGPT. We are using the library and resources from the HuggingFace platform to facilitate access and manipulation of these models.

## Available Notebooks

1. **EleutherAI_gpt-neo-2.7B.ipynb**
   - **Description**: This notebook uses the GPT-Neo model with 2.7 billion parameters, developed by EleutherAI. GPT-Neo is an open and accessible implementation of a language model similar to OpenAI's GPT-3. It is capable of generating coherent and relevant text based on user-provided inputs.
   - **Objective**: Demonstrate the ability of GPT-Neo to generate high-quality text and explore the practical applications of this model in natural language processing (NLP) tasks.

2. **GPT2.ipynb**
   - **Description**: This notebook explores the GPT-2 model, one of the first large-scale models developed by OpenAI. With different sizes available (124M, 355M, 774M, 1.5B), GPT-2 revolutionized the field of NLP by showing that models trained with large amounts of data can generate text with an impressive level of coherence.
   - **Objective**: Investigate the capabilities of GPT-2 in generating text and understand how the increase in the number of parameters impacts the model's performance.

3. **pierrreguillou_gpt2-small-portuguese.ipynb**
   - **Description**: This notebook uses an adapted version of GPT-2 for the Portuguese language, developed by Pierre Guillou. This adaptation allows the model to generate text in Portuguese with a quality comparable to English models, demonstrating the versatility and applicability of such models in different languages.
   - **Objective**: Evaluate the performance of GPT-2 in Portuguese and understand the challenges and benefits of adapting language models to different languages.

## Resources Used

- **HuggingFace Library**: We are using the `transformers` library from HuggingFace, which provides a simple and powerful interface for using pre-trained language models. This library facilitates access to a wide range of models and allows us to fine-tune and perform detailed evaluations of their capabilities.
- **HuggingFace Platform**: In addition to the library, the HuggingFace platform offers a robust infrastructure for sharing and running models, enabling efficient collaboration and reproducibility of experiments.

## Educational Objectives

The main objective of these notebooks is to provide a solid foundation on how language generation models work and how they can be applied in various contexts. By exploring these examples, students will be able to:
- Understand the basic concepts of deep neural networks applied to language generation.
- Evaluate the differences between models of different sizes and complexities.
- Apply pre-trained models to practical NLP tasks.
- Adapt and fine-tune language models for different languages and contexts.

This knowledge is essential for understanding and using larger models like ChatGPT, allowing students to advance in their research and practical applications in the field of artificial intelligence.

We hope that this repository will be a valuable tool in learning about language models and inspire continued exploration in this dynamic and rapidly evolving field.
