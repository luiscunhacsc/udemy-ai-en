{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgRcm8VVN8+U3PWW3WWsJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiscunhacsc/udemy-ai-en/blob/main/Transformer2017KerasNLP_MostAdvaanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Uninstall existing versions if necessary\n",
        "#!pip uninstall -y tensorflow keras keras_nlp\n",
        "\n",
        "\n",
        "# Step 2: Install compatible versions of TensorFlow and KerasNLP\n",
        "#!pip install tensorflow==2.15 keras_nlp==0.3.0"
      ],
      "metadata": {
        "id": "BfORulDBqE_z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "ZOl8v-08pkf1",
        "outputId": "00f68f36-dbd7-4bb6-8fc0-e746e0ab7a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1487/1487 [==============================] - ETA: 0s - loss: 1.9694 - accuracy: 0.7912\n",
            "Epoch 1 translations:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'query' (type EinsumDense).\n\n{{function_node __wrapped__Einsum_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected input 0 to have rank 3 but got: 4 [Op:Einsum] name: \n\nCall arguments received by layer 'query' (type EinsumDense):\n  • inputs=tf.Tensor(shape=(1, 64, 20, 128), dtype=float32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-95ab7a21aabf>\u001b[0m in \u001b[0;36m<cell line: 166>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0mtranslation_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslationCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtranslation_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;31m# Summary of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-95ab7a21aabf>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0menglish_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menglish_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mspanish_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspanish_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mtranslated_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"English: {' '.join([english_vectorizer.get_vocabulary()[index] for index in english_sentence if index > 0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target Spanish: {' '.join([self.index_to_word[index] for index in spanish_sentence if index > 0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-95ab7a21aabf>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0mpredicted_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0moutput_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/layers/transformer_encoder.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, padding_mask, attention_mask)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# Self attention.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         attended = self._multi_head_attention_layer(\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         )\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'query' (type EinsumDense).\n\n{{function_node __wrapped__Einsum_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected input 0 to have rank 3 but got: 4 [Op:Einsum] name: \n\nCall arguments received by layer 'query' (type EinsumDense):\n  • inputs=tf.Tensor(shape=(1, 64, 20, 128), dtype=float32)"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "# Step 3: Download the dataset\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip',\n",
        "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.join(os.path.dirname(path_to_zip), 'spa-eng', 'spa.txt')\n",
        "\n",
        "# Step 4: Load and prepare the dataset\n",
        "with open(path_to_file, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "# Split the lines into English and Spanish pairs\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    if '\\t' in line:\n",
        "        english, spanish = line.split('\\t')\n",
        "        text_pairs.append((english, spanish))\n",
        "\n",
        "# Convert to numpy arrays for easier handling\n",
        "english_texts, spanish_texts = zip(*text_pairs)\n",
        "english_texts = np.array(english_texts)\n",
        "spanish_texts = np.array(spanish_texts)\n",
        "\n",
        "# Step 5: Tokenize and Vectorize the data\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "# Vectorize English text\n",
        "english_vectorizer = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "english_vectorizer.adapt(english_texts)\n",
        "\n",
        "# Vectorize Spanish text\n",
        "spanish_vectorizer = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length + 1)\n",
        "\n",
        "spanish_vectorizer.adapt(spanish_texts)\n",
        "\n",
        "def vectorize_text(eng, spa):\n",
        "    eng = english_vectorizer(eng)\n",
        "    spa = spanish_vectorizer(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1]}, spa[:, 1:])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((english_texts, spanish_texts))\n",
        "dataset = dataset.batch(64)\n",
        "dataset = dataset.map(vectorize_text)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(list(dataset)))\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "# Step 6: Build the Transformer Model\n",
        "\n",
        "def build_encoder(num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate):\n",
        "    inputs = tf.keras.Input(shape=(None,), dtype=tf.int64, name='encoder_inputs')\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        vocabulary_size=input_vocab_size,\n",
        "        sequence_length=sequence_length,\n",
        "        embedding_dim=d_model\n",
        "    )(inputs)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        x = keras_nlp.layers.TransformerEncoder(\n",
        "            intermediate_dim=dff,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout_rate\n",
        "        )(x)\n",
        "\n",
        "    return tf.keras.Model(inputs, x, name='encoder')\n",
        "\n",
        "def build_decoder(num_layers, d_model, num_heads, dff, target_vocab_size, dropout_rate):\n",
        "    inputs = tf.keras.Input(shape=(None,), dtype=tf.int64, name='decoder_inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        vocabulary_size=target_vocab_size,\n",
        "        sequence_length=sequence_length,\n",
        "        embedding_dim=d_model\n",
        "    )(inputs)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        x = keras_nlp.layers.TransformerDecoder(\n",
        "            intermediate_dim=dff,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout_rate\n",
        "        )(decoder_sequence=x, encoder_sequence=enc_outputs)\n",
        "\n",
        "    outputs = layers.Dense(target_vocab_size, activation='softmax')(x)\n",
        "    return tf.keras.Model([inputs, enc_outputs], outputs, name='decoder')\n",
        "\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "dropout_rate = 0.1\n",
        "\n",
        "encoder = build_encoder(num_layers, d_model, num_heads, dff, vocab_size, dropout_rate)\n",
        "decoder = build_decoder(num_layers, d_model, num_heads, dff, vocab_size, dropout_rate)\n",
        "\n",
        "# Define the input layers\n",
        "encoder_inputs = encoder.input\n",
        "decoder_inputs = decoder.input[0]\n",
        "decoder_outputs = decoder([decoder_inputs, encoder.output])\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Custom callback to display translations after each epoch\n",
        "class TranslationCallback(Callback):\n",
        "    def __init__(self, model, val_data, num_examples=5):\n",
        "        self.model = model\n",
        "        self.val_data = val_data.take(num_examples)\n",
        "        self.english_texts = [ex[0]['encoder_inputs'].numpy() for ex in self.val_data]\n",
        "        self.spanish_texts = [ex[1].numpy() for ex in self.val_data]\n",
        "        self.index_to_word = {i: word for i, word in enumerate(spanish_vectorizer.get_vocabulary())}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"\\nEpoch {epoch+1} translations:\")\n",
        "        for i in range(len(self.english_texts)):\n",
        "            english_sentence = self.english_texts[i]\n",
        "            spanish_sentence = self.spanish_texts[i]\n",
        "            translated_sentence = self.translate_sentence(english_sentence)\n",
        "            print(f\"English: {' '.join([english_vectorizer.get_vocabulary()[index] for index in english_sentence if index > 0])}\")\n",
        "            print(f\"Target Spanish: {' '.join([self.index_to_word[index] for index in spanish_sentence if index > 0])}\")\n",
        "            print(f\"Predicted Spanish: {' '.join(translated_sentence)}\\n\")\n",
        "\n",
        "    def translate_sentence(self, sentence):\n",
        "        encoder_input = tf.expand_dims(sentence, axis=0)\n",
        "        decoder_input = tf.expand_dims([spanish_vectorizer('<start>')], 0)\n",
        "        output_sentence = []\n",
        "\n",
        "        for i in range(sequence_length):\n",
        "            predictions = self.model([encoder_input, decoder_input], training=False)\n",
        "            predicted_id = tf.argmax(predictions[0, -1, :]).numpy()\n",
        "            output_sentence.append(predicted_id)\n",
        "\n",
        "            # Stop prediction if end token is predicted\n",
        "            if predicted_id == spanish_vectorizer('<end>'):\n",
        "                break\n",
        "\n",
        "            decoder_input = tf.concat([decoder_input, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
        "\n",
        "        return [self.index_to_word[id] for id in output_sentence if id > 0 and id != spanish_vectorizer('<end>')]\n",
        "\n",
        "# Step 7: Train the model with the callback\n",
        "translation_callback = TranslationCallback(model, val_dataset, num_examples=5)\n",
        "\n",
        "model.fit(train_dataset, epochs=20, validation_data=val_dataset, callbacks=[translation_callback])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ]
    }
  ]
}